{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python361064bitpy36conda4f32697db52543acaedb8346d569b572",
   "display_name": "Python 3.6.10 64-bit ('py36': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# ä½¿ç”¨huggingfaceå…¨å®¶æ¡¶(transformers, datasets)å®ç°ä¸€æ¡é¾™BERTè®­ç»ƒ(trainer)å’Œé¢„æµ‹(pipeline)\n",
    "\n",
    "huggingfaceçš„[transformers](https://github.com/huggingface/transformers)åœ¨æˆ‘å†™ä¸‹æœ¬æ–‡æ—¶å·²æœ‰39.5k starï¼Œå¯èƒ½æ˜¯ç›®å‰æœ€æµè¡Œçš„æ·±åº¦å­¦ä¹ åº“äº†ï¼Œè€Œè¿™å®¶æœºæ„åˆæä¾›äº†[datasets](https://github.com/huggingface/datasets)è¿™ä¸ªåº“ï¼Œå¸®åŠ©å¿«é€Ÿè·å–å’Œå¤„ç†æ•°æ®ã€‚è¿™ä¸€å¥—å…¨å®¶æ¡¶ä½¿å¾—æ•´ä¸ªä½¿ç”¨BERTç±»æ¨¡å‹æœºå™¨å­¦ä¹ æµç¨‹å˜å¾—å‰æ‰€æœªæœ‰çš„ç®€å•ã€‚\n",
    "\n",
    "ä¸è¿‡ï¼Œç›®å‰æˆ‘åœ¨ç½‘ä¸Šæ²¡æœ‰å‘ç°æ¯”è¾ƒç®€å•çš„å…³äºæ•´ä¸ªä¸€å¥—å…¨å®¶æ¡¶çš„ä½¿ç”¨æ•™ç¨‹ã€‚æ‰€ä»¥å†™ä¸‹æ­¤æ–‡ï¼Œå¸Œæœ›å¸®åŠ©æ›´å¤šäººå¿«é€Ÿä¸Šæ‰‹ã€‚\n",
    "\n",
    "è¿™é‡Œï¼Œæˆ‘ä»¬ä»¥`AGNews`æ–°é—»åˆ†ç±»ä»»åŠ¡ä¸ºä¾‹ï¼Œæ¼”ç¤ºæ•´å¥—æµç¨‹çš„å®ç°ã€‚"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"  # åœ¨æ­¤æˆ‘æŒ‡å®šä½¿ç”¨2å·GPUï¼Œå¯æ ¹æ®éœ€è¦è°ƒæ•´\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "source": [
    "## ä½¿ç”¨datasetsè¯»å–æ•°æ®é›†\n",
    "\n",
    "ä¸‹é¢çš„ä»£ç è¯»å–åŸå§‹æ•°æ®é›†çš„trainéƒ¨åˆ†çš„å‰40000æ¡ä½œä¸ºæˆ‘ä»¬çš„è®­ç»ƒé›†ï¼Œ40000-50000æ¡ä½œä¸ºå¼€å‘é›†ï¼ˆåªä½¿ç”¨è¿™ä¸ªå­é›†å·²ç»å¯ä»¥è®­å‡ºä¸é”™çš„æ¨¡å‹ï¼Œå¹¶ä¸”å¯ä»¥è®©è®­ç»ƒæ—¶é—´æ›´çŸ­ï¼‰ï¼ŒåŸå§‹çš„æµ‹è¯•é›†ä½œä¸ºæˆ‘ä»¬çš„æµ‹è¯•é›†ã€‚"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (/home/zhiling/.cache/huggingface/datasets/ag_news/default/0.0.0/fb5c5e74a110037311ef5e904583ce9f8b9fbc1354290f97b4929f01b3f48b1a)\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (/home/zhiling/.cache/huggingface/datasets/ag_news/default/0.0.0/fb5c5e74a110037311ef5e904583ce9f8b9fbc1354290f97b4929f01b3f48b1a)\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (/home/zhiling/.cache/huggingface/datasets/ag_news/default/0.0.0/fb5c5e74a110037311ef5e904583ce9f8b9fbc1354290f97b4929f01b3f48b1a)\n",
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 40000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 10000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 7600\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "train_dataset = load_dataset(\"ag_news\", split=\"train[:40000]\")\n",
    "dev_dataset = load_dataset(\"ag_news\", split=\"train[40000:50000]\")\n",
    "test_dataset = load_dataset(\"ag_news\", split=\"test\")\n",
    "print(train_dataset)\n",
    "print(dev_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "source": [
    "åŸå§‹æ•°æ®é›†åŒ…å«textå’Œlabelä¸¤ä¸ªå­—æ®µ"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'text': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(num_classes=4, names=['World', 'Sports', 'Business', 'Sci/Tech'], names_file=None, id=None)}"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "train_dataset.features"
   ]
  },
  {
   "source": [
    "ç”±äºbertæ¨¡å‹æœŸæœ›å¾—åˆ°çš„æ ‡ç­¾çš„å­—æ®µä¸º`labels`è€ŒåŸå§‹æ•°æ®é›†ä¸­çš„åå­—æ˜¯`label`ï¼Œæ‰€ä»¥åšä¸€ä¸‹è°ƒæ•´ã€‚\n",
    "\n",
    "ä¸‹é¢çš„ä»£ç æŠŠ`label`å­—æ®µå¤åˆ¶åˆ°`labels`ã€‚"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Loading cached processed dataset at /home/zhiling/.cache/huggingface/datasets/ag_news/default/0.0.0/fb5c5e74a110037311ef5e904583ce9f8b9fbc1354290f97b4929f01b3f48b1a/cache-fe78f98825d1041c.arrow\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'label': 2,\n",
       " 'labels': 2,\n",
       " 'text': \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\"}"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(lambda examples: {'labels': examples['label']}, batched=True)\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Loading cached processed dataset at /home/zhiling/.cache/huggingface/datasets/ag_news/default/0.0.0/fb5c5e74a110037311ef5e904583ce9f8b9fbc1354290f97b4929f01b3f48b1a/cache-a66ed2efed56e4c9.arrow\n",
      "Loading cached processed dataset at /home/zhiling/.cache/huggingface/datasets/ag_news/default/0.0.0/fb5c5e74a110037311ef5e904583ce9f8b9fbc1354290f97b4929f01b3f48b1a/cache-5f9134046515027e.arrow\n"
     ]
    }
   ],
   "source": [
    "dev_dataset = dev_dataset.map(lambda examples: {'labels': examples['label']}, batched=True)\n",
    "test_dataset = test_dataset.map(lambda examples: {'labels': examples['label']}, batched=True)"
   ]
  },
  {
   "source": [
    "## åŠ è½½æ¨¡å‹ï¼Œtokenizerï¼Œå¹¶é¢„å¤„ç†æ•°æ®\n",
    "\n",
    "ä¸ºäº†å¿«é€Ÿå®éªŒï¼Œæˆ‘ä»¬é€‰æ‹©ä¸€ä¸ªè¾ƒå°çš„`bert-tiny`æ¨¡å‹è¿›è¡Œå®éªŒã€‚åŠ è½½å¯¹åº”æ¨¡å‹å’Œtokenizer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_id = 'prajjwal1/bert-tiny'\n",
    "# note that we need to specify the number of classes for this task\n",
    "# we can directly use the metadata (num_classes) stored in the dataset\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id, \n",
    "            num_labels=train_dataset.features[\"label\"].num_classes)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "source": [
    "ç”¨bertçš„æ–¹æ³•å¯¹æ•°æ®é›†åšåˆ†è¯é¢„å¤„ç†ï¼ŒæŠŠæ‰€æœ‰åºåˆ—è¡¥å……æˆ–æˆªæ–­åˆ°256ä¸ªtoken"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=40.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "49b10b93f84e4e78bc0a2fee2dbff4d2"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9764e9a80b73409eb152b936c99048ea"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Loading cached processed dataset at /home/zhiling/.cache/huggingface/datasets/ag_news/default/0.0.0/fb5c5e74a110037311ef5e904583ce9f8b9fbc1354290f97b4929f01b3f48b1a/cache-0ece2b3c35fe711f.arrow\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 256\n",
    "train_dataset = train_dataset.map(lambda e: tokenizer(e['text'], truncation=True, padding='max_length', max_length=MAX_LENGTH), batched=True)\n",
    "dev_dataset = dev_dataset.map(lambda e: tokenizer(e['text'], truncation=True, padding='max_length', max_length=MAX_LENGTH), batched=True)\n",
    "test_dataset = test_dataset.map(lambda e: tokenizer(e['text'], truncation=True, padding='max_length', max_length=MAX_LENGTH), batched=True)"
   ]
  },
  {
   "source": [
    "ä¸ºäº†æ”¾è¿›pytorchæ¨¡å‹è®­ç»ƒï¼Œè¿˜è¦å†å£°æ˜æ ¼å¼å’Œä½¿ç”¨çš„å­—æ®µ"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n",
    "dev_dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n",
    "test_dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "source": [
    "ç°åœ¨æˆ‘ä»¬çš„è®­ç»ƒæ ·æœ¬é•¿è¿™æ ·ï¼Œå¯ä»¥ç›´æ¥æ”¾è¿›bertè®­ç»ƒäº†"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'attention_mask': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n",
       " 'input_ids': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n",
       " 'label': ClassLabel(num_classes=4, names=['World', 'Sports', 'Business', 'Sci/Tech'], names_file=None, id=None),\n",
       " 'labels': Value(dtype='int64', id=None),\n",
       " 'text': Value(dtype='string', id=None),\n",
       " 'token_type_ids': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "train_dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'input_ids': tensor([  101,  2813,  2358,  1012,  6468, 15020,  2067,  2046,  1996,  2304,\n",
       "          1006, 26665,  1007, 26665,  1011,  2460,  1011, 19041,  1010,  2813,\n",
       "          2395,  1005,  1055,  1040, 11101,  2989,  1032,  2316,  1997, 11087,\n",
       "          1011, 22330,  8713,  2015,  1010,  2024,  3773,  2665,  2153,  1012,\n",
       "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0]),\n",
       " 'labels': tensor(2),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "source": [
    "æˆ‘ä»¬å¯ä»¥æŒ‡å®šæ¨¡å‹è®­ç»ƒæ—¶ï¼Œæ˜¾ç¤ºçš„éªŒè¯æŒ‡æ ‡"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "source": [
    "æŒ‡å®šè®­ç»ƒå‚æ•°ï¼Œä½¿ç”¨trainerç›´æ¥è®­ç»ƒ"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n        <style>\n            /* Turns off some styling */\n            progress {\n                /* gets rid of default border in Firefox and Opera. */\n                border: none;\n                /* Needs to be in here for Safari polyfill so background images work as expected. */\n                background-size: auto;\n            }\n        </style>\n      \n      <progress value='2' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   2/1875 : < :, Epoch 0.00/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    learning_rate=3e-4,\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=64,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=100,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    no_cuda=False,\n",
    "    load_best_model_at_end=True,\n",
    "    # eval_steps=100,\n",
    "    evaluation_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated ğŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=dev_dataset,            # evaluation dataset\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "train_out = trainer.train()"
   ]
  },
  {
   "source": [
    "## ä½¿ç”¨pipelineç›´æ¥å¯¹æ–‡æœ¬è¿›è¡Œé¢„æµ‹\n",
    "\n",
    "`pipeline`å¯ä»¥ç›´æ¥åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹å’Œtokenizerï¼Œç„¶åç›´æ¥å¯¹æ–‡æœ¬è¿›è¡Œåˆ†ç±»é¢„æµ‹ï¼Œæ— éœ€å†è‡ªè¡Œé¢„å¤„ç†\n",
    "\n",
    "é¦–å…ˆæˆ‘ä»¬æŠŠæ¨¡å‹æ”¾å›cpuæ¥è¿›è¡Œé¢„æµ‹"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cpu()"
   ]
  },
  {
   "source": [
    "ç”¨`sentiment-analysis`æ¥æŒ‡å®šæˆ‘ä»¬åšçš„æ˜¯æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ï¼ˆæƒ…æ„Ÿåˆ†ææ˜¯ä¸€ç±»ä»£è¡¨æ€§çš„æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ï¼‰ï¼Œå¹¶æŒ‡å®šæˆ‘ä»¬ä¹‹å‰è®­å¥½çš„æ¨¡å‹ã€‚"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "æˆ‘ä»¬ä»æ¨¡å‹æ²¡æœ‰è§è¿‡çš„testé›†é‡ŒæŒ‘ä¸€ä¸ªä¾‹å­æ¥è¿›è¡Œé¢„æµ‹"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (/home/zhiling/.cache/huggingface/datasets/ag_news/default/0.0.0/fb5c5e74a110037311ef5e904583ce9f8b9fbc1354290f97b4929f01b3f48b1a)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'label': 2,\n",
       " 'text': \"Fears for T N pension after talks Unions representing workers at Turner   Newall say they are 'disappointed' after talks with stricken parent firm Federal Mogul.\"}"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "test_examples = load_dataset(\"ag_news\", split=\"test[:10]\")\n",
    "test_examples[0]"
   ]
  },
  {
   "source": [
    "è¯¥æ–‡æœ¬çš„ç±»åˆ«ä¸º2ï¼Œçœ‹çœ‹æ¨¡å‹èƒ½ä¸èƒ½åšå‡ºæ­£ç¡®é¢„æµ‹ï¼Ÿ"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_2', 'score': 0.9601152539253235}]"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "result = classifier(test_examples[0]['text'])\n",
    "result"
   ]
  },
  {
   "source": [
    "é¢„æµ‹æ­£ç¡®ï¼\n",
    "\n",
    "åˆ°æ­¤æˆ‘ä»¬çš„huggingfaceå…¨å®¶æ¡¶å°±å¤§åŠŸå‘Šæˆäº†~\n",
    "\n",
    "æœ¬æ–‡çš„å®Œå…¨ä»£ç å¯ä»¥ç›´æ¥åœ¨è¿™é‡Œæ‰¾åˆ°ï¼š[https://github.com/blmoistawinde/hello\\_world/blob/master/huggingface\\_classification.ipynb](https://github.com/blmoistawinde/hello_world/blob/master/huggingface_classification.ipynb)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}